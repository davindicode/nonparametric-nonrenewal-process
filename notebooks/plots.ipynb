{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import scipy.special as sps\n",
    "import scipy.stats as scstats\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import patches\n",
    "from matplotlib import transforms\n",
    "import daft\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../../neuroprob\")\n",
    "sys.path.append(\"../scripts/\") # access to scripts\n",
    "\n",
    "\n",
    "import os\n",
    "if not os.path.exists('./output'):\n",
    "    os.makedirs('./output')\n",
    "    \n",
    "\n",
    "from neuroprob import utils\n",
    "\n",
    "import model_utils\n",
    "\n",
    "import pickle\n",
    "\n",
    "\n",
    "plt.style.use(['paper.mplstyle'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render(pgm):\n",
    "    \"\"\"\n",
    "    Wrapper for rendering PGM via daft\n",
    "    \"\"\"\n",
    "    for plate in pgm._plates:\n",
    "        plate.render(pgm._ctx)\n",
    "\n",
    "    for edge in pgm._edges:\n",
    "        edge.render(pgm._ctx)\n",
    "\n",
    "    for name in pgm._nodes:\n",
    "        pgm._nodes[name].render(pgm._ctx)\n",
    "\n",
    "\n",
    "\n",
    "def init_figax(pgm, fig, ax):\n",
    "    \"\"\"\n",
    "    Wrapper for initializing PGM via daft\n",
    "    \"\"\"\n",
    "    pgm._ctx._figure = fig\n",
    "    ax.axis('off')\n",
    "\n",
    "    # Set the bounds.\n",
    "    l0 = pgm._ctx.convert(*pgm._ctx.origin)\n",
    "    l1 = pgm._ctx.convert(*(pgm._ctx.origin + pgm._ctx.shape))\n",
    "    ax.set_xlim(l0[0], l1[0])\n",
    "    ax.set_ylim(l0[1], l1[1])\n",
    "    ax.set_aspect(1)\n",
    "\n",
    "    pgm._ctx._ax = ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic process over point process conditionals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.randn(10, 100)*5.+10.0\n",
    "dt = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tl = 3000\n",
    "sample_bin = 0.001\n",
    "\n",
    "\n",
    "l = sample_bin*np.array([30.0])[None, :]\n",
    "dn = l.shape[1]\n",
    "v = np.ones(dn)\n",
    "\n",
    "\n",
    "\n",
    "# generate GP trajectories\n",
    "kernel_tuples = [('variance', v), \n",
    "                 ('RBF', 'euclid', l)]\n",
    "\n",
    "with torch.no_grad():\n",
    "    kernel, _, _ = GP.kernels.create_kernel(kernel_tuples, 'softplus', torch.double)\n",
    "\n",
    "    inp = torch.arange(Tl)[None, None, :, None]*sample_bin\n",
    "    K = kernel(inp, inp)[0, ...]\n",
    "    K.view(dn, -1)[:, ::Tl+1] += 1e-6\n",
    "\n",
    "\n",
    "L = torch.cholesky(K)\n",
    "mc = 10\n",
    "xi = np.random.randn(mc, dn, Tl)\n",
    "x = (L[None, ...] * xi[..., None, :]).sum(-1)\n",
    "\n",
    "\n",
    "\n",
    "# linear Poisson model\n",
    "neurons = 100\n",
    "w_len = dn\n",
    "GPFA = mdl.parametrics.GLM(w_len, neurons, w_len, 'exp', bias=True)\n",
    "\n",
    "w = np.random.randn(neurons, w_len)\n",
    "bias = np.random.randn(neurons)\n",
    "GPFA.set_params(w, bias)\n",
    "\n",
    "\n",
    "likelihood = mdl.likelihoods.Poisson(sample_bin, neurons, 'exp')\n",
    "#likelihood.set_Y(rc_t, batch_size=5000, filter_len=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x.numpy()[1, 0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.exp(x)\n",
    "p = x.numpy()[:, 0, :]*np.exp(-np.cumsum(x.numpy()[:, 0, :], axis=1)*dt) # natural time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau = np.arange(Tl)*dt\n",
    "tau_0 = 0.001\n",
    "t = tau_0*(np.exp(tau) - 1)\n",
    "dtau_dt = 1/(t+tau_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(tau, p.mean(0).T)\n",
    "plt.plot(tau[:, None].repeat(p.shape[0], axis=1), p.T, alpha=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(t, p.mean(0).T*dtau_dt)\n",
    "plt.plot(t[:, None].repeat(p.shape[0], axis=1), p.T*dtau_dt[:, None], alpha=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(p*dt).sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neurons = 1\n",
    "hist_couple = None\n",
    "shape_t = 5.0*np.ones(neurons)\n",
    "likelihood = mdl.likelihoods.Gamma(sample_bin, neurons, rate_model.inv_link, shape_t)\n",
    "#sigma_t = 0.5*np.ones(neurons)\n",
    "#likelihood = mdl.likelihoods.logNormal(sample_bin, neurons, inv_link, sigma_t)\n",
    "\n",
    "#mu_t = 5.0*np.ones(neurons)\n",
    "#likelihood = mdl.likelihoods.invGaussian(neurons, inv_link, mu_t)\n",
    "\n",
    "#hist_len = 99 # 100 steps of spiketrain, no instantaneous element\n",
    "#hist_couple = mdl.filters.raised_cosine_bumps(a=1., c=1., phi=phi_h, w=w_h, timesteps=hist_len)\n",
    "#likelihood = mdl.likelihoods.Bernoulli(neurons, inv_link)\n",
    "\n",
    "#input_group = mdl.inference.input_group(3, [(None, None, None, 1)]*3)\n",
    "#input_group.set_XZ(covariates, track_samples, batch_size=track_samples, trials=trials)\n",
    "\n",
    "#glm = mdl.inference.VI_optimized(input_group, gauss_rate, likelihood)\n",
    "#glm.to(dev)\n",
    "    \n",
    "\n",
    "bb_isi = np.linspace(0.001, 5.0, 100) # ISI evaluate\n",
    "\n",
    "#gt_field.append(compute_rate(model.rate_model[0], [0])[0])\n",
    "ISI = [torch.tensor(bb_isi[:, None], device='cpu')]#dev)]\n",
    "scale = likelihood.shape.data\n",
    "#scale = torch.exp(-model.likelihood.sigma.data**2/2.)\n",
    "#scale = 1./model.likelihood.mu\n",
    "likelihood.trials = 1\n",
    "p = likelihood.nll(torch.log(scale)*torch.ones((len(bb_isi), 1), device='cpu'), [[ISI[0]*scale]], [0])#.data[:, 0].cpu().numpy()\n",
    "a = np.exp(-p)\n",
    "\n",
    "b = likelihood.ISI_dist([0]).prob_dens(bb_isi)\n",
    "\n",
    "plt.plot(bb_isi, a)\n",
    "plt.plot(bb_isi, b, 'r--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### monotonic RQ splines ###\n",
    "def searchsorted(bin_locations, inputs, eps=1e-6):\n",
    "    bin_locations[..., -1] += eps\n",
    "    return torch.sum(inputs[..., None] >= bin_locations, dim=-1) - 1\n",
    "\n",
    "\n",
    "def RQS(\n",
    "    inputs,\n",
    "    unnormalized_widths,\n",
    "    unnormalized_heights,\n",
    "    unnormalized_derivatives,\n",
    "    inverse=False,\n",
    "    left=0.0,\n",
    "    right=1.0,\n",
    "    bottom=0.0,\n",
    "    top=1.0,\n",
    "    min_bin_width=1e-3,\n",
    "    min_bin_height=1e-3,\n",
    "    min_derivative=1e-3,\n",
    "):\n",
    "    \"\"\"\n",
    "    last dimension of params is number of bins\n",
    "    first dimensions are (batch,) or (batch, dims)\n",
    "\n",
    "    Based on implementation in https://github.com/bayesiains/nsf\n",
    "    \"\"\"\n",
    "    if torch.min(inputs) < left or torch.max(inputs) > right:\n",
    "        raise ValueError(\"Input outside domain\")\n",
    "\n",
    "    num_bins = unnormalized_widths.shape[-1]\n",
    "\n",
    "    if min_bin_width * num_bins > 1.0:\n",
    "        raise ValueError(\"Minimal bin width too large for the number of bins\")\n",
    "    if min_bin_height * num_bins > 1.0:\n",
    "        raise ValueError(\"Minimal bin height too large for the number of bins\")\n",
    "\n",
    "    widths = nn.functional.softmax(unnormalized_widths, dim=-1)\n",
    "    widths = min_bin_width + (1 - min_bin_width * num_bins) * widths\n",
    "    cumwidths = torch.cumsum(widths, dim=-1)\n",
    "    cumwidths = nn.functional.pad(cumwidths, pad=(1, 0), mode=\"constant\", value=0.0)\n",
    "    cumwidths = (right - left) * cumwidths + left\n",
    "    cumwidths[..., 0] = left\n",
    "    cumwidths[..., -1] = right\n",
    "    widths = cumwidths[..., 1:] - cumwidths[..., :-1]\n",
    "\n",
    "    derivatives = min_derivative + nn.functional.softplus(unnormalized_derivatives)\n",
    "\n",
    "    heights = nn.functional.softmax(unnormalized_heights, dim=-1)\n",
    "    heights = min_bin_height + (1 - min_bin_height * num_bins) * heights\n",
    "    cumheights = torch.cumsum(heights, dim=-1)\n",
    "    cumheights = nn.functional.pad(cumheights, pad=(1, 0), mode=\"constant\", value=0.0)\n",
    "    cumheights = (top - bottom) * cumheights + bottom\n",
    "    cumheights[..., 0] = bottom\n",
    "    cumheights[..., -1] = top\n",
    "    heights = cumheights[..., 1:] - cumheights[..., :-1]\n",
    "\n",
    "    if inverse:\n",
    "        bin_idx = searchsorted(cumheights, inputs)[..., None]\n",
    "    else:\n",
    "        bin_idx = searchsorted(cumwidths, inputs)[..., None]\n",
    "\n",
    "    input_cumwidths = cumwidths.gather(-1, bin_idx)[..., 0]\n",
    "    input_bin_widths = widths.gather(-1, bin_idx)[..., 0]\n",
    "\n",
    "    input_cumheights = cumheights.gather(-1, bin_idx)[..., 0]\n",
    "    delta = heights / widths\n",
    "    input_delta = delta.gather(-1, bin_idx)[..., 0]\n",
    "\n",
    "    input_derivatives = derivatives.gather(-1, bin_idx)[..., 0]\n",
    "    input_derivatives_plus_one = derivatives[..., 1:].gather(-1, bin_idx)\n",
    "    input_derivatives_plus_one = input_derivatives_plus_one[..., 0]\n",
    "\n",
    "    input_heights = heights.gather(-1, bin_idx)[..., 0]\n",
    "\n",
    "    if inverse:\n",
    "        a = (inputs - input_cumheights) * (\n",
    "            input_derivatives + input_derivatives_plus_one - 2 * input_delta\n",
    "        ) + input_heights * (input_delta - input_derivatives)\n",
    "        b = input_heights * input_derivatives - (inputs - input_cumheights) * (\n",
    "            input_derivatives + input_derivatives_plus_one - 2 * input_delta\n",
    "        )\n",
    "        c = -input_delta * (inputs - input_cumheights)\n",
    "\n",
    "        discriminant = b.pow(2) - 4 * a * c\n",
    "        assert (discriminant >= 0).all()\n",
    "\n",
    "        root = (2 * c) / (-b - torch.sqrt(discriminant))\n",
    "        outputs = root * input_bin_widths + input_cumwidths\n",
    "\n",
    "        theta_one_minus_theta = root * (1 - root)\n",
    "        denominator = input_delta + (\n",
    "            (input_derivatives + input_derivatives_plus_one - 2 * input_delta)\n",
    "            * theta_one_minus_theta\n",
    "        )\n",
    "        derivative_numerator = input_delta.pow(2) * (\n",
    "            input_derivatives_plus_one * root.pow(2)\n",
    "            + 2 * input_delta * theta_one_minus_theta\n",
    "            + input_derivatives * (1 - root).pow(2)\n",
    "        )\n",
    "        logdetjac = torch.log(derivative_numerator) - 2 * torch.log(denominator)\n",
    "        return outputs, -logdetjac\n",
    "\n",
    "    else:\n",
    "        theta = (inputs - input_cumwidths) / input_bin_widths\n",
    "        theta_one_minus_theta = theta * (1 - theta)\n",
    "\n",
    "        numerator = input_heights * (\n",
    "            input_delta * theta.pow(2) + input_derivatives * theta_one_minus_theta\n",
    "        )\n",
    "        denominator = input_delta + (\n",
    "            (input_derivatives + input_derivatives_plus_one - 2 * input_delta)\n",
    "            * theta_one_minus_theta\n",
    "        )\n",
    "        outputs = input_cumheights + numerator / denominator\n",
    "\n",
    "        derivative_numerator = input_delta.pow(2) * (\n",
    "            input_derivatives_plus_one * theta.pow(2)\n",
    "            + 2 * input_delta * theta_one_minus_theta\n",
    "            + input_derivatives * (1 - theta).pow(2)\n",
    "        )\n",
    "        logdetjac = torch.log(derivative_numerator) - 2 * torch.log(denominator)\n",
    "        return outputs, logdetjac\n",
    "\n",
    "\n",
    "def unconstrained_RQS(\n",
    "    inputs,\n",
    "    unnormalized_widths,\n",
    "    unnormalized_heights,\n",
    "    unnormalized_derivatives,\n",
    "    inverse=False,\n",
    "    tail_bound=1.0,\n",
    "    min_bin_width=1e-3,\n",
    "    min_bin_height=1e-3,\n",
    "    min_derivative=1e-3,\n",
    "):\n",
    "    \"\"\"\n",
    "    Based on implementation in https://github.com/bayesiains/nsf\n",
    "    \"\"\"\n",
    "    outputs = torch.zeros_like(inputs)\n",
    "    logdetjac = torch.zeros_like(inputs)\n",
    "\n",
    "    unnormalized_derivatives = nn.functional.pad(unnormalized_derivatives, pad=(1, 1))\n",
    "    constant = math.log(math.exp(1 - min_derivative) - 1)\n",
    "    unnormalized_derivatives[..., 0] = constant\n",
    "    unnormalized_derivatives[..., -1] = constant\n",
    "\n",
    "    inside_intvl_mask = (inputs >= -tail_bound) & (inputs <= tail_bound)\n",
    "    outside_interval_mask = ~inside_intvl_mask\n",
    "\n",
    "    if outside_interval_mask.sum() > 0:\n",
    "        outputs[outside_interval_mask] = inputs[outside_interval_mask]\n",
    "        logdetjac[outside_interval_mask] = 0\n",
    "\n",
    "    if inside_intvl_mask.sum() > 0:\n",
    "        outputs[inside_intvl_mask], logdetjac[inside_intvl_mask] = RQS(\n",
    "            inputs=inputs[inside_intvl_mask],\n",
    "            unnormalized_widths=unnormalized_widths[inside_intvl_mask, :],\n",
    "            unnormalized_heights=unnormalized_heights[inside_intvl_mask, :],\n",
    "            unnormalized_derivatives=unnormalized_derivatives[inside_intvl_mask, :],\n",
    "            inverse=inverse,\n",
    "            left=-tail_bound,\n",
    "            right=tail_bound,\n",
    "            bottom=-tail_bound,\n",
    "            top=tail_bound,\n",
    "            min_bin_width=min_bin_width,\n",
    "            min_bin_height=min_bin_height,\n",
    "            min_derivative=min_derivative,\n",
    "        )\n",
    "\n",
    "    return outputs, logdetjac\n",
    "\n",
    "\n",
    "class NRQS(nn.Module):\n",
    "    \"\"\"\n",
    "    Neural rational quadratic spline flow, coupling layer\n",
    "\n",
    "    [Durkan et al. 2019]\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mask1, f1, f2, K, B):\n",
    "        \"\"\"\n",
    "        f1 and f2 take in full input x and map out flattened (3 * K - 1) * dims\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.dim1 = mask1.sum()\n",
    "        self.dim2 = np.prod(mask1.shape) - self.dim1\n",
    "        self.K = K\n",
    "        self.B = B\n",
    "\n",
    "        self.register_buffer(\"mask1\", mask1.type(torch.bool))  # add batch dimension\n",
    "        self.register_buffer(\"mask2\", ~mask1.type(torch.bool))\n",
    "        self.f1 = f1  # output (3 * K - 1) * dim2\n",
    "        self.f2 = f2\n",
    "\n",
    "    def _compute_RQS(self, x, W, H, D, inverse):\n",
    "        W = 2 * self.B * torch.softmax(W, dim=2)\n",
    "        H = 2 * self.B * torch.softmax(H, dim=2)\n",
    "        D = nn.functional.softplus(D)\n",
    "        x, ld = unconstrained_RQS(x, W, H, D, inverse, tail_bound=self.B)\n",
    "        return x, ld\n",
    "\n",
    "    def forward(self, x, reverse=False, log_px=0):\n",
    "        \"\"\"\n",
    "        :param torch.tensor x: input of shape (batch, dims)\n",
    "        \"\"\"\n",
    "        x1, x2 = x[:, self.mask1], x[:, self.mask2]\n",
    "        x = torch.clone(x)  # copy to avoid overwriting input\n",
    "\n",
    "        if reverse:\n",
    "            x2_ = x * self.mask2\n",
    "            out = self.f2(x2_).reshape(-1, self.dim1, 3 * self.K - 1)\n",
    "            W, H, D = torch.split(out, self.K, dim=2)\n",
    "            x1, ld = self._compute_RQS(x1, W, H, D, inverse=True)\n",
    "            x[:, self.mask1] = x1  # update x1 part\n",
    "            log_px = log_px + torch.sum(ld, dim=1)\n",
    "\n",
    "            x1_ = x * self.mask1\n",
    "            out = self.f1(x1_).reshape(-1, self.dim2, 3 * self.K - 1)\n",
    "            W, H, D = torch.split(out, self.K, dim=2)\n",
    "            x2, ld = self._compute_RQS(x2, W, H, D, inverse=True)\n",
    "            x[:, self.mask2] = x2  # update x2 part\n",
    "            log_px = log_px + torch.sum(ld, dim=1)\n",
    "\n",
    "        else:\n",
    "            x1_ = x * self.mask1\n",
    "            out = self.f1(x1_).reshape(-1, self.dim2, 3 * self.K - 1)\n",
    "            W, H, D = torch.split(out, self.K, dim=2)\n",
    "            x2, ld = self._compute_RQS(x2, W, H, D, inverse=False)\n",
    "            x[:, self.mask2] = x2  # update x2 part\n",
    "            log_px = log_px + torch.sum(ld, dim=1)\n",
    "\n",
    "            x2_ = x * self.mask2\n",
    "            out = self.f2(x2_).reshape(-1, self.dim1, 3 * self.K - 1)\n",
    "            W, H, D = torch.split(out, self.K, dim=2)\n",
    "            x1, ld = self._compute_RQS(x1, W, H, D, inverse=False)\n",
    "            x[:, self.mask1] = x1  # update x1 part\n",
    "            log_px = log_px + torch.sum(ld, dim=1)\n",
    "\n",
    "        return x, log_px\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schematic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,5)) # plot fits\n",
    "fig.text(-0.21, 1.16, 'A', transform=ax.transAxes, size=15)\n",
    "\n",
    "time_bins = I_ext[0].shape[0]\n",
    "tt = np.arange(time_bins)*dt\n",
    "\n",
    "\n",
    "\n",
    "widths = [1]\n",
    "heights = [1, 1, 1, 2]\n",
    "spec = fig.add_gridspec(ncols=1, nrows=4, width_ratios=widths, height_ratios=heights, \n",
    "                        left=0., right=0.7, bottom=0., top=1.0)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jr\n",
    "from jax.nn import softplus\n",
    "\n",
    "import optax\n",
    "\n",
    "jax.config.update('jax_platform_name', 'cpu')\n",
    "#jax.config.update('jax_disable_jit', True)\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "#import os\n",
    "#if not os.path.exists('./saves'):\n",
    "#    os.makedirs('./saves')\n",
    "\n",
    "\n",
    "import pickle\n",
    "\n",
    "import lib\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(['../paper.mplstyle'])\n",
    "\n",
    "\n",
    "import os\n",
    "if not os.path.exists('./output'):\n",
    "    os.makedirs('./output')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render(pgm):\n",
    "    \"\"\"\n",
    "    Wrapper for rendering PGM via daft\n",
    "    \"\"\"\n",
    "    for plate in pgm._plates:\n",
    "        plate.render(pgm._ctx)\n",
    "\n",
    "    for edge in pgm._edges:\n",
    "        edge.render(pgm._ctx)\n",
    "\n",
    "    for name in pgm._nodes:\n",
    "        pgm._nodes[name].render(pgm._ctx)\n",
    "\n",
    "\n",
    "\n",
    "def init_figax(pgm, fig, ax):\n",
    "    \"\"\"\n",
    "    Wrapper for initializing PGM via daft\n",
    "    \"\"\"\n",
    "    pgm._ctx._figure = fig\n",
    "    ax.axis('off')\n",
    "\n",
    "    # Set the bounds.\n",
    "    l0 = pgm._ctx.convert(*pgm._ctx.origin)\n",
    "    l1 = pgm._ctx.convert(*(pgm._ctx.origin + pgm._ctx.shape))\n",
    "    ax.set_xlim(l0[0], l1[0])\n",
    "    ax.set_ylim(l0[1], l1[1])\n",
    "    ax.set_aspect(1)\n",
    "\n",
    "    pgm._ctx._ax = ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic process over point process conditionals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'GP' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-798cad4a0133>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mkernel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGP\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_kernel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel_tuples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'softplus'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdouble\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0minp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msample_bin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'GP' is not defined"
     ]
    }
   ],
   "source": [
    "def get_stat_model(mu, lsigma, log_beta, log_gamma, len_x, kernel_class, neurons):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    f_dims = neurons\n",
    "    x_dims = f_dims\n",
    "    \n",
    "    v_t = 1.*np.ones(neurons)\n",
    "    v_r = -.3*np.ones(neurons)\n",
    "    tau_m = 0.01*np.ones(neurons)\n",
    "    tau_s = np.ones(neurons)\n",
    "    log_beta = log_beta*np.ones(neurons)\n",
    "    log_gamma = log_gamma*np.ones(neurons)\n",
    "    \n",
    "    tau_h = 0.1*np.ones(neurons)\n",
    "    a = 1.0*np.ones(neurons)\n",
    "    b = 1000.0*np.ones(neurons)\n",
    "\n",
    "    IF_model = lib.IF_models.LIF(log_beta, log_gamma, v_t, v_r, tau_s, tau_m)#, tau_h, a, b)\n",
    "    \n",
    "    eps_mapping = lib.mappings.Identity(f_dims)\n",
    "\n",
    "    ### state space GP ###\n",
    "    var_x = 1.0*np.ones(x_dims)  # GP variance\n",
    "    len_x = len_x*np.ones((x_dims, 1))  # GP lengthscale\n",
    "\n",
    "    kernx = kernel_class(x_dims, variance=var_x, lengthscale=len_x)\n",
    "    #kernx = lib.kernels.IID(jnp.eye(x_dims))\n",
    "    \n",
    "    state_space = lib.GP.state_space.FullLDS(kernx, diagonal_site=True)\n",
    "\n",
    "    ### mu and sigma ###\n",
    "    params = {'value': mu*jnp.ones((f_dims,))}\n",
    "    mu_mapping = lib.mappings.Constant(x_dims, f_dims, params)\n",
    "\n",
    "    params = {'value': lsigma*jnp.ones((f_dims,))}\n",
    "    lsigma_mapping = lib.mappings.Constant(x_dims, f_dims, params)\n",
    "    \n",
    "    ### initial conditions ###\n",
    "    q_vh_ic = jnp.zeros((num_samps, neurons, IF_model.q_d))\n",
    "    \n",
    "    ### IF GP model ###\n",
    "    model = lib.inference.IF_SSGP(\n",
    "        state_space, eps_mapping, mu_mapping, lsigma_mapping, IF_model, q_vh_ic, dtype=jnp.float64)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neurons = 2\n",
    "num_samps = 5\n",
    "prng_state = jax.random.PRNGKey(123)\n",
    "\n",
    "model = get_stat_model(\n",
    "    0.1, -0.5, 3., 3., 0.01, lib.GP.kernels.Matern12, neurons)\n",
    "\n",
    "\n",
    "Tsteps = 1000\n",
    "t = np.linspace(0., 5., Tsteps)  # s\n",
    "x_obs = jnp.empty((t.shape[0], 1, 0))\n",
    "y = np.zeros((t.shape[0], neurons))\n",
    "#mask = np.ones_like(y).astype(bool)\n",
    "#mask[:, 0] = False\n",
    "mask = None\n",
    "model.set_data(t, x_obs, y, mask=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_dims' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m### state space GP ###\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m var_x \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m\u001b[38;5;241m*\u001b[39mnp\u001b[38;5;241m.\u001b[39mones(\u001b[43mx_dims\u001b[49m)  \u001b[38;5;66;03m# GP variance\u001b[39;00m\n\u001b[1;32m      3\u001b[0m len_x \u001b[38;5;241m=\u001b[39m len_x\u001b[38;5;241m*\u001b[39mnp\u001b[38;5;241m.\u001b[39mones((x_dims, \u001b[38;5;241m1\u001b[39m))  \u001b[38;5;66;03m# GP lengthscale\u001b[39;00m\n\u001b[1;32m      5\u001b[0m kernx \u001b[38;5;241m=\u001b[39m kernel_class(x_dims, variance\u001b[38;5;241m=\u001b[39mvar_x, lengthscale\u001b[38;5;241m=\u001b[39mlen_x)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'x_dims' is not defined"
     ]
    }
   ],
   "source": [
    "f_dims = neurons\n",
    "x_dims = f_dims\n",
    "\n",
    "### state space GP ###\n",
    "var_x = 1.0*np.ones(x_dims)  # GP variance\n",
    "len_x = len_x*np.ones((x_dims, 1))  # GP lengthscale\n",
    "\n",
    "kernx = kernel_class(x_dims, variance=var_x, lengthscale=len_x)\n",
    "#kernx = lib.kernels.IID(jnp.eye(x_dims))\n",
    "\n",
    "state_space = lib.GP.state_space.FullLDS(kernx, diagonal_site=True)\n",
    "\n",
    "\n",
    "\n",
    "eps_samples = self.eps_mapping.sample_prior(\n",
    "                eps_mapping, prng_keys[0], x_samples, jitter)\n",
    "\n",
    "eps_samples, _ = self.eps_mapping.sample_posterior(\n",
    "    eps_mapping, eps_var_params, prng_keys[0], x_samples, jitter, False)  # (time, tr, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Tsteps = 1000\n",
    "t = np.linspace(0., 5., Tsteps)  # s\n",
    "x_obs = jnp.empty((t.shape[0], 1, 0))\n",
    "tr = 0\n",
    "y = np.array(spiketrains[:, tr, :])\n",
    "#mask = np.ones_like(y).astype(bool)\n",
    "#mask[:, 0] = False\n",
    "mask = None\n",
    "model.set_data(t, x_obs, y, mask=mask)\n",
    "\n",
    "\n",
    "\n",
    "model.state_space.normal_site_init(model.t.shape[0], std=1.0)\n",
    "model.state_space.var_params['site_obs'] = model.state_space.var_params['site_obs'].at[500, :, 0].set(0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_posterior(t_eval, timedata, params, var_params, mean_only, compute_KL, jitter):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.exp(x)\n",
    "p = x.numpy()[:, 0, :]*np.exp(-np.cumsum(x.numpy()[:, 0, :], axis=1)*dt) # natural time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.arange(1000)\n",
    "params = {\"t0\": 100.}\n",
    "\n",
    "tau, dtau_dt = time_transform(params, t, inverse=False)\n",
    "t_, dt_dtau = time_transform(params, tau, inverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fca90311bb0>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 245x183.75 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(t, tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(tau, p.mean(0).T)\n",
    "plt.plot(tau[:, None].repeat(p.shape[0], axis=1), p.T, alpha=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(t, p.mean(0).T*dtau_dt)\n",
    "plt.plot(t[:, None].repeat(p.shape[0], axis=1), p.T*dtau_dt[:, None], alpha=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(p*dt).sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neurons = 1\n",
    "hist_couple = None\n",
    "shape_t = 5.0*np.ones(neurons)\n",
    "likelihood = mdl.likelihoods.Gamma(sample_bin, neurons, rate_model.inv_link, shape_t)\n",
    "#sigma_t = 0.5*np.ones(neurons)\n",
    "#likelihood = mdl.likelihoods.logNormal(sample_bin, neurons, inv_link, sigma_t)\n",
    "\n",
    "#mu_t = 5.0*np.ones(neurons)\n",
    "#likelihood = mdl.likelihoods.invGaussian(neurons, inv_link, mu_t)\n",
    "\n",
    "#hist_len = 99 # 100 steps of spiketrain, no instantaneous element\n",
    "#hist_couple = mdl.filters.raised_cosine_bumps(a=1., c=1., phi=phi_h, w=w_h, timesteps=hist_len)\n",
    "#likelihood = mdl.likelihoods.Bernoulli(neurons, inv_link)\n",
    "\n",
    "#input_group = mdl.inference.input_group(3, [(None, None, None, 1)]*3)\n",
    "#input_group.set_XZ(covariates, track_samples, batch_size=track_samples, trials=trials)\n",
    "\n",
    "#glm = mdl.inference.VI_optimized(input_group, gauss_rate, likelihood)\n",
    "#glm.to(dev)\n",
    "    \n",
    "\n",
    "bb_isi = np.linspace(0.001, 5.0, 100) # ISI evaluate\n",
    "\n",
    "#gt_field.append(compute_rate(model.rate_model[0], [0])[0])\n",
    "ISI = [torch.tensor(bb_isi[:, None], device='cpu')]#dev)]\n",
    "scale = likelihood.shape.data\n",
    "#scale = torch.exp(-model.likelihood.sigma.data**2/2.)\n",
    "#scale = 1./model.likelihood.mu\n",
    "likelihood.trials = 1\n",
    "p = likelihood.nll(torch.log(scale)*torch.ones((len(bb_isi), 1), device='cpu'), [[ISI[0]*scale]], [0])#.data[:, 0].cpu().numpy()\n",
    "a = np.exp(-p)\n",
    "\n",
    "b = likelihood.ISI_dist([0]).prob_dens(bb_isi)\n",
    "\n",
    "plt.plot(bb_isi, a)\n",
    "plt.plot(bb_isi, b, 'r--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def searchsorted(bin_locations, inputs, eps=1e-6):\n",
    "    bin_locations[..., -1] += eps\n",
    "    return torch.sum(inputs[..., None] >= bin_locations, dim=-1) - 1\n",
    "\n",
    "\n",
    "def RQS(\n",
    "    inputs: torch.Tensor,\n",
    "    unnormalized_widths: torch.Tensor,\n",
    "    unnormalized_heights: torch.Tensor,\n",
    "    unnormalized_derivatives: torch.Tensor,\n",
    "    inverse: bool,\n",
    "    left: float,\n",
    "    right: float,\n",
    "    bottom: float,\n",
    "    top: float,\n",
    "    min_bin_width: float,\n",
    "    min_bin_height: float,\n",
    "    min_derivative: float,\n",
    "):\n",
    "    \"\"\"\n",
    "    last dimension of params is number of bins\n",
    "    first dimensions are (batch,) or (batch, dims)\n",
    "\n",
    "    Based on implementation in https://github.com/bayesiains/nsf\n",
    "    \"\"\"\n",
    "    if torch.min(inputs) < left or torch.max(inputs) > right:\n",
    "        raise ValueError(\"Input outside domain\")\n",
    "\n",
    "    num_bins = unnormalized_widths.shape[-1]\n",
    "\n",
    "    if min_bin_width * num_bins > 1.0:\n",
    "        raise ValueError(\"Minimal bin width too large for the number of bins\")\n",
    "    if min_bin_height * num_bins > 1.0:\n",
    "        raise ValueError(\"Minimal bin height too large for the number of bins\")\n",
    "\n",
    "    widths = nn.functional.softmax(unnormalized_widths, dim=-1)\n",
    "    widths = min_bin_width + (1 - min_bin_width * num_bins) * widths\n",
    "    cumwidths = torch.cumsum(widths, dim=-1)\n",
    "    cumwidths = nn.functional.pad(cumwidths, pad=(1, 0), mode=\"constant\", value=0.0)\n",
    "    cumwidths = (right - left) * cumwidths + left\n",
    "    cumwidths[..., 0] = left\n",
    "    cumwidths[..., -1] = right\n",
    "    widths = cumwidths[..., 1:] - cumwidths[..., :-1]\n",
    "\n",
    "    derivatives = min_derivative + nn.functional.softplus(unnormalized_derivatives)\n",
    "\n",
    "    heights = nn.functional.softmax(unnormalized_heights, dim=-1)\n",
    "    heights = min_bin_height + (1 - min_bin_height * num_bins) * heights\n",
    "    cumheights = torch.cumsum(heights, dim=-1)\n",
    "    cumheights = nn.functional.pad(cumheights, pad=(1, 0), mode=\"constant\", value=0.0)\n",
    "    cumheights = (top - bottom) * cumheights + bottom\n",
    "    cumheights[..., 0] = bottom\n",
    "    cumheights[..., -1] = top\n",
    "    heights = cumheights[..., 1:] - cumheights[..., :-1]\n",
    "\n",
    "    if inverse:\n",
    "        bin_idx = searchsorted(cumheights, inputs)[..., None]\n",
    "    else:\n",
    "        bin_idx = searchsorted(cumwidths, inputs)[..., None]\n",
    "\n",
    "    input_cumwidths = cumwidths.gather(-1, bin_idx)[..., 0]\n",
    "    input_bin_widths = widths.gather(-1, bin_idx)[..., 0]\n",
    "\n",
    "    input_cumheights = cumheights.gather(-1, bin_idx)[..., 0]\n",
    "    delta = heights / widths\n",
    "    input_delta = delta.gather(-1, bin_idx)[..., 0]\n",
    "\n",
    "    input_derivatives = derivatives.gather(-1, bin_idx)[..., 0]\n",
    "    input_derivatives_plus_one = derivatives[..., 1:].gather(-1, bin_idx)\n",
    "    input_derivatives_plus_one = input_derivatives_plus_one[..., 0]\n",
    "\n",
    "    input_heights = heights.gather(-1, bin_idx)[..., 0]\n",
    "\n",
    "    if inverse:\n",
    "        a = (inputs - input_cumheights) * (\n",
    "            input_derivatives + input_derivatives_plus_one - 2 * input_delta\n",
    "        ) + input_heights * (input_delta - input_derivatives)\n",
    "        b = input_heights * input_derivatives - (inputs - input_cumheights) * (\n",
    "            input_derivatives + input_derivatives_plus_one - 2 * input_delta\n",
    "        )\n",
    "        c = -input_delta * (inputs - input_cumheights)\n",
    "\n",
    "        discriminant = b.pow(2) - 4 * a * c\n",
    "        if (discriminant >= 0).all() is False:\n",
    "            raise RuntimeError('Rational quadratic spline inverse discriminant negative')\n",
    "\n",
    "        root = (2 * c) / (-b - torch.sqrt(discriminant))\n",
    "        outputs = root * input_bin_widths + input_cumwidths\n",
    "\n",
    "        theta_one_minus_theta = root * (1 - root)\n",
    "        denominator = input_delta + (\n",
    "            (input_derivatives + input_derivatives_plus_one - 2 * input_delta)\n",
    "            * theta_one_minus_theta\n",
    "        )\n",
    "        derivative_numerator = input_delta.pow(2) * (\n",
    "            input_derivatives_plus_one * root.pow(2)\n",
    "            + 2 * input_delta * theta_one_minus_theta\n",
    "            + input_derivatives * (1 - root).pow(2)\n",
    "        )\n",
    "        logdetjac = torch.log(derivative_numerator) - 2 * torch.log(denominator)\n",
    "        return outputs, -logdetjac\n",
    "\n",
    "    else:\n",
    "        theta = (inputs - input_cumwidths) / input_bin_widths\n",
    "        theta_one_minus_theta = theta * (1 - theta)\n",
    "\n",
    "        numerator = input_heights * (\n",
    "            input_delta * theta.pow(2) + input_derivatives * theta_one_minus_theta\n",
    "        )\n",
    "        denominator = input_delta + (\n",
    "            (input_derivatives + input_derivatives_plus_one - 2 * input_delta)\n",
    "            * theta_one_minus_theta\n",
    "        )\n",
    "        outputs = input_cumheights + numerator / denominator\n",
    "\n",
    "        derivative_numerator = input_delta.pow(2) * (\n",
    "            input_derivatives_plus_one * theta.pow(2)\n",
    "            + 2 * input_delta * theta_one_minus_theta\n",
    "            + input_derivatives * (1 - theta).pow(2)\n",
    "        )\n",
    "        logdetjac = torch.log(derivative_numerator) - 2 * torch.log(denominator)\n",
    "        return outputs, logdetjac\n",
    "\n",
    "\n",
    "def unconstrained_RQS(\n",
    "    inputs: torch.Tensor,\n",
    "    unnormalized_widths: torch.Tensor,\n",
    "    unnormalized_heights: torch.Tensor,\n",
    "    unnormalized_derivatives: torch.Tensor,\n",
    "    inverse: bool,\n",
    "    tail_bound: float,\n",
    "    min_bin_width: float,\n",
    "    min_bin_height: float,\n",
    "    min_derivative: float,\n",
    "):\n",
    "    \"\"\"\n",
    "    Based on implementation in https://github.com/bayesiains/nsf\n",
    "    \"\"\"\n",
    "    outputs = torch.zeros_like(inputs)\n",
    "    logdetjac = torch.zeros_like(inputs)\n",
    "\n",
    "    unnormalized_derivatives = nn.functional.pad(unnormalized_derivatives, pad=(1, 1))\n",
    "    constant = math.log(math.exp(1 - min_derivative) - 1)\n",
    "    unnormalized_derivatives[..., 0] = constant\n",
    "    unnormalized_derivatives[..., -1] = constant\n",
    "\n",
    "    inside_intvl_mask = (inputs >= -tail_bound) & (inputs <= tail_bound)\n",
    "    outside_interval_mask = ~inside_intvl_mask\n",
    "\n",
    "    if outside_interval_mask.sum() > 0:\n",
    "        outputs[outside_interval_mask] = inputs[outside_interval_mask]\n",
    "        logdetjac[outside_interval_mask] = 0\n",
    "\n",
    "    if inside_intvl_mask.sum() > 0:\n",
    "        outputs[inside_intvl_mask], logdetjac[inside_intvl_mask] = RQS(\n",
    "            inputs=inputs[inside_intvl_mask],\n",
    "            unnormalized_widths=unnormalized_widths[inside_intvl_mask, :],\n",
    "            unnormalized_heights=unnormalized_heights[inside_intvl_mask, :],\n",
    "            unnormalized_derivatives=unnormalized_derivatives[inside_intvl_mask, :],\n",
    "            inverse=inverse,\n",
    "            left=-tail_bound,\n",
    "            right=tail_bound,\n",
    "            bottom=-tail_bound,\n",
    "            top=tail_bound,\n",
    "            min_bin_width=min_bin_width,\n",
    "            min_bin_height=min_bin_height,\n",
    "            min_derivative=min_derivative,\n",
    "        )\n",
    "\n",
    "    return outputs, logdetjac\n",
    "\n",
    "\n",
    "class NRQS(nn.Module):\n",
    "    \"\"\"\n",
    "    Neural rational quadratic spline flow, coupling layer\n",
    "\n",
    "    [Durkan et al. 2019]\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mask1, f1, f2, K, B):\n",
    "        \"\"\"\n",
    "        f1 and f2 take in full input x and map out flattened (3 * K - 1) * dims\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.dim1 = mask1.sum()\n",
    "        self.dim2 = np.prod(mask1.shape) - self.dim1\n",
    "        self.K = K\n",
    "        self.B = B\n",
    "\n",
    "        self.register_buffer(\"mask1\", mask1.type(torch.bool))  # add batch dimension\n",
    "        self.register_buffer(\"mask2\", ~mask1.type(torch.bool))\n",
    "        self.f1 = f1  # output (3 * K - 1) * dim2\n",
    "        self.f2 = f2\n",
    "\n",
    "    def _compute_RQS(self, x, W, H, D, inverse):\n",
    "        W = 2 * self.B * torch.softmax(W, dim=2)\n",
    "        H = 2 * self.B * torch.softmax(H, dim=2)\n",
    "        D = nn.functional.softplus(D)\n",
    "        x, ld = unconstrained_RQS(\n",
    "            x, W, H, D, inverse, tail_bound=self.B, \n",
    "            min_bin_width=1e-2, min_bin_height=1e-2, min_derivative=1e-2, \n",
    "        )\n",
    "        return x, ld\n",
    "\n",
    "    def forward(self, x, reverse=False, log_px=0):\n",
    "        \"\"\"\n",
    "        :param torch.tensor x: input of shape (batch, dims)\n",
    "        \"\"\"\n",
    "        x1, x2 = x[:, self.mask1], x[:, self.mask2]\n",
    "        x = torch.clone(x)  # copy to avoid overwriting input\n",
    "\n",
    "        if reverse:\n",
    "            x2_ = x * self.mask2\n",
    "            out = self.f2(x2_).reshape(-1, self.dim1, 3 * self.K - 1)\n",
    "            W, H, D = torch.split(out, self.K, dim=2)\n",
    "            x1, ld = self._compute_RQS(x1, W, H, D, inverse=True)\n",
    "            x[:, self.mask1] = x1  # update x1 part\n",
    "            log_px = log_px + torch.sum(ld, dim=1)\n",
    "\n",
    "            x1_ = x * self.mask1\n",
    "            out = self.f1(x1_).reshape(-1, self.dim2, 3 * self.K - 1)\n",
    "            W, H, D = torch.split(out, self.K, dim=2)\n",
    "            x2, ld = self._compute_RQS(x2, W, H, D, inverse=True)\n",
    "            x[:, self.mask2] = x2  # update x2 part\n",
    "            log_px = log_px + torch.sum(ld, dim=1)\n",
    "\n",
    "        else:\n",
    "            x1_ = x * self.mask1\n",
    "            out = self.f1(x1_).reshape(-1, self.dim2, 3 * self.K - 1)\n",
    "            W, H, D = torch.split(out, self.K, dim=2)\n",
    "            x2, ld = self._compute_RQS(x2, W, H, D, inverse=False)\n",
    "            x[:, self.mask2] = x2  # update x2 part\n",
    "            log_px = log_px + torch.sum(ld, dim=1)\n",
    "\n",
    "            x2_ = x * self.mask2\n",
    "            out = self.f2(x2_).reshape(-1, self.dim1, 3 * self.K - 1)\n",
    "            W, H, D = torch.split(out, self.K, dim=2)\n",
    "            x1, ld = self._compute_RQS(x1, W, H, D, inverse=False)\n",
    "            x[:, self.mask1] = x1  # update x1 part\n",
    "            log_px = log_px + torch.sum(ld, dim=1)\n",
    "\n",
    "        return x, log_px\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schematic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,5)) # plot fits\n",
    "fig.text(-0.21, 1.16, 'A', transform=ax.transAxes, size=15)\n",
    "\n",
    "time_bins = I_ext[0].shape[0]\n",
    "tt = np.arange(time_bins)*dt\n",
    "\n",
    "\n",
    "\n",
    "widths = [1]\n",
    "heights = [1, 1, 1, 2]\n",
    "spec = fig.add_gridspec(ncols=1, nrows=4, width_ratios=widths, height_ratios=heights, \n",
    "                        left=0., right=0.7, bottom=0., top=1.0)\n",
    "\n",
    "ax = fig.add_subplot(spec[0, 0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python39",
   "language": "python",
   "name": "python39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

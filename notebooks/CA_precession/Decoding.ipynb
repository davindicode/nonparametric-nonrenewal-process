{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 1.7.0\n",
      "Using device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../..\")\n",
    "\n",
    "\n",
    "import neuroprob as mdl\n",
    "from neuroprob import utils\n",
    "\n",
    "import utils_func\n",
    "\n",
    "dev = utils.pytorch.get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading data\n",
    "dataset = 'hc3'\n",
    "session_id = 'ec014.468'\n",
    "\n",
    "data = np.load('./checkpoint/{}_{}.npz'.format(dataset, session_id))\n",
    "spktrain = data['spktrain']\n",
    "x_t = data['x_t']\n",
    "y_t = data['y_t']\n",
    "hd_t = data['hd_t']\n",
    "theta_t = data['theta_t']\n",
    "eeg_t = data['eeg_t']\n",
    "sample_bin = data['sample_bin']\n",
    "\n",
    "units = spktrain.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# incompleted runs:  1\n"
     ]
    }
   ],
   "source": [
    "c_x_t = utils_func.class_x_t(x_t)\n",
    "dir_t = utils_func.L_R_run(c_x_t)\n",
    "ind_L_R = np.where(dir_t == -1)\n",
    "ind_R_L = np.where(dir_t == 1)\n",
    "ind_stat = np.where(dir_t == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_spikes_L_R = np.sum(spktrain[:, ind_L_R], axis=(1, 2), dtype=int)\n",
    "n_spikes_R_L = np.sum(spktrain[:, ind_R_L], axis=(1, 2), dtype=int)\n",
    "n_spikes_stat = np.sum(spktrain[:, ind_stat], axis=(1, 2), dtype=int)\n",
    "n_spikes = np.vstack((n_spikes_L_R, n_spikes_stat, n_spikes_R_L))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_len = 1\n",
    "\n",
    "rp_t = np.stack((rhd_t,))\n",
    "x_dims = rp_t.shape[0]\n",
    "\n",
    "cov_hist = tools.lagged_input(torch.tensor(rp_t).float(), hist_len=hist_len, hist_stride=1, time_stride=1)\n",
    "act_hist = tools.lagged_input(torch.tensor(rc_t).float(), hist_len=hist_len, hist_stride=1, time_stride=1) # dim, time, hist\n",
    "\n",
    "\n",
    "batch_size = 1000\n",
    "\n",
    "# inputs\n",
    "dec_input = act_hist.permute(1, 0, 2).float() # time, neurons, hist\n",
    "enc_input = cov_hist.permute(1, 0, 2).float()\n",
    "\n",
    "# targets\n",
    "cov_batched = torch.split(torch.tensor(rp_t[:, hist_len-1:].T).float(), batch_size)\n",
    "\n",
    "\n",
    "if hist_len > 1:\n",
    "    dec_input_batched = torch.split(dec_input[:-hist_len+1, ...], batch_size)\n",
    "    enc_input_batched = torch.split(enc_input[:-hist_len+1, ...], batch_size)\n",
    "    act_batched = torch.split(torch.tensor(rc_t[:, :-hist_len+1].T).float(), batch_size)\n",
    "else:\n",
    "    dec_input_batched = torch.split(dec_input, batch_size)\n",
    "    enc_input_batched = torch.split(enc_input, batch_size)\n",
    "    act_batched = torch.split(torch.tensor(rc_t.T).float(), batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_dist = [distributions.Tn_Normal]\n",
    "muMLP = tools.MLP([50, 20, 20, 30], neurons, lat_dims, nonlin=tools.Siren(), out=None)\n",
    "sigmaMLP = tools.MLP([50, 20, 20, 30], neurons, lat_dims, nonlin=tools.Siren(), out=nn.Softplus())\n",
    "sigmaMLP.net[-2].weight.data.fill_(0.)\n",
    "sigmaMLP.net[-2].bias.data.fill_(0.)\n",
    "kern = torch.ones((neurons, hist_len))/hist_len\n",
    "\n",
    "rate_dec = utils.decoding.rate_decoder(muMLP, sigmaMLP, kern)\n",
    "rate_dec.to(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "optimizer = torch.optim.Adam(list(rate_dec.parameters()), lr=1e-4, weight_decay=0.0)\n",
    "sch = optim.lr_scheduler.MultiplicativeLR(optimizer, lambda e: 0.9)\n",
    "print(\"Number of parameters: {}\".format(sum(p.numel() for p in rate_dec.parameters())))\n",
    "\n",
    "loss = utils.decoding.fit_decoder(rate_dec, x_dist, dec_input_batched, cov_batched, resamples, optimizer, sch, 100, \n",
    "                              dev, 10000)\n",
    "\n",
    "plt.plot(loss)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

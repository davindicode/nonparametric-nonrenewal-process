{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 1.7.1+cu101\n",
      "Using device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../..\")\n",
    "\n",
    "\n",
    "import neuroprob as mdl\n",
    "from neuroprob import utils\n",
    "\n",
    "import utils_func\n",
    "\n",
    "dev = utils.pytorch.get_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains classical analysis of linear track data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Mattijs\\\\Documents\\\\Mattijs\\\\Engineering IIB\\\\Thesis\\\\neural-data-analysis\\\\notebook\\\\CA_precession'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Mattijs\\\\Documents\\\\Mattijs\\\\Engineering IIB\\\\Thesis\\\\neural-data-analysis'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir('..')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading data\n",
    "dataset = 'hc3'\n",
    "session_id = 'ec014.468'\n",
    "\n",
    "data = np.load('./saves/{}_{}.npz'.format(dataset, session_id))\n",
    "spktrain = data['spktrain']\n",
    "x_t = data['x_t']\n",
    "y_t = data['y_t']\n",
    "hd_t = data['hd_t']\n",
    "theta_t = data['theta_t']\n",
    "eeg_t = data['eeg_t']\n",
    "sample_bin = data['sample_bin']\n",
    "\n",
    "units = spktrain.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_x = x_t.min()\n",
    "right_x = x_t.max()\n",
    "bottom_y = y_t.min()\n",
    "top_y = y_t.max()\n",
    "\n",
    "arena_width = right_x - left_x\n",
    "arena_height = top_y - bottom_y\n",
    "\n",
    "track_samples = len(x_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vx_t = (x_t[1:]-x_t[:-1])/sample_bin\n",
    "vy_t = (y_t[1:]-y_t[:-1])/sample_bin\n",
    "vx_t = np.concatenate((vx_t, vx_t[-1:]))\n",
    "vy_t = np.concatenate((vy_t, vy_t[-1:]))\n",
    "s_t = np.sqrt(vx_t**2 + vy_t**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sep_t_spike = []\n",
    "for k in range(units):\n",
    "    sep_t_spike.append(utils.neural.binned_to_indices(spktrain[k]))\n",
    "    \n",
    "ISI, LV = utils.neural.compute_ISI_LV(sample_bin, sep_t_spike) # compute ISIs and bursting coefficient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate runs: L-R, R-L, stationary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# incompleted runs:  1\n",
      "% L_R ind: 10.62406005172688%\n",
      "% R_L ind: 13.714040059631877%\n",
      "% end/stationary ind: 75.66189988864124%\n"
     ]
    }
   ],
   "source": [
    "c_x_t = utils_func.class_x_t(x_t)\n",
    "dir_t = utils_func.L_R_run(c_x_t)\n",
    "ind_L_R = np.where(dir_t == -1)\n",
    "ind_R_L = np.where(dir_t == 1)\n",
    "ind_stat = np.where(dir_t == 0)\n",
    "print(\"% L_R ind: {}%\".format(len(ind_L_R[0]) / len(dir_t)*100))\n",
    "print(\"% R_L ind: {}%\".format(len(ind_R_L[0]) / len(dir_t)*100))\n",
    "print(\"% end/stationary ind: {}%\".format(len(ind_stat[0]) / len(dir_t)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select units by number of spikes and sparsity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# binning of covariates and analysis\n",
    "bins_x = 40\n",
    "bins_y = int(arena_height/arena_width*bins_x)\n",
    "bins_theta = 20\n",
    "\n",
    "bin_x = np.linspace(left_x, right_x+1e-3, bins_x+1)\n",
    "bin_y = np.linspace(bottom_y, top_y+1e-3, bins_y+1)\n",
    "bin_theta = np.linspace(0, 2*np.pi+1e-3, bins_theta+1)\n",
    "\n",
    "#corr_t_spikes = utils.neural.spike_threshold(sample_bin, 0.25, (x_t, y_t), (bin_x, bin_y), sep_t_spike)\n",
    "# selection of bins > 0.25 already happens in IPP_model here for convenience\n",
    "# Mutual Informations\n",
    "sp_rate, sp_prob = utils.neural.IPP_model(sample_bin, 0.25, (x_t, y_t), (bin_x, bin_y), \n",
    "                                          sep_t_spike, divide=True)\n",
    "sp_MI  = utils.neural.spike_var_MI(sp_rate, sp_prob)\n",
    "\n",
    "sm_size = 5\n",
    "sm_filter = np.ones((sm_size, sm_size)) / sm_size**2\n",
    "smth_rate = utils.neural.smooth_hist(sp_rate, sm_filter, ['repeat', 'repeat'], dev='cpu')\n",
    "coherence, sparsity = utils.neural.geometric_tuning(sp_rate, smth_rate, sp_prob) # coherence and sparsity of fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_spikes_L_R = np.sum(spktrain[:, ind_L_R], axis=(1, 2), dtype=int)\n",
    "n_spikes_R_L = np.sum(spktrain[:, ind_R_L], axis=(1, 2), dtype=int)\n",
    "n_spikes_stat = np.sum(spktrain[:, ind_stat], axis=(1, 2), dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# discarded units (< 200 spikes):  39\n",
      "[ 2  6  7 13 15 16 17 18 19 22 23 27 29 30 32 40 45 46 47 48 49 50 52 54\n",
      " 58 60 61 62 65 67 72 73 74 75 78 80 81 82 90]\n",
      "Units left:  57\n"
     ]
    }
   ],
   "source": [
    "unit_discard = np.intersect1d(np.where(n_spikes_L_R < 200), np.where(n_spikes_R_L < 200))\n",
    "print(\"# discarded units (< 200 spikes): \", len(unit_discard))\n",
    "print(unit_discard)\n",
    "unit_used = np.union1d(np.where(n_spikes_L_R > 200), np.where(n_spikes_R_L > 200))\n",
    "print(\"Units left: \", len(unit_used))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Units left:  39\n"
     ]
    }
   ],
   "source": [
    "# Discard neurons if not sparse, enough (inhibitory neurons always ON)\n",
    "unit_used = np.intersect1d(unit_used, np.argsort(sparsity)[18:])\n",
    "print(\"Units left: \", len(unit_used))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Units left:  15\n"
     ]
    }
   ],
   "source": [
    "# delete some more if needed, chosen by histogram\n",
    "del_mask = np.isin(unit_used, [5, 12, 25, 36, 39, 59, 76, 77, 85, 0, 8, 9, 24, 26, 28, 31, 35, 38, 51, 53, 57, 87, 88, 41])\n",
    "unit_used = np.delete(unit_used, np.where(del_mask))\n",
    "print(\"Units left: \", len(unit_used))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save reduced dataset\n",
    "np.savez_compressed('./checkpoint/{}_{}_reduced.npz'.format(dataset, session_id), \n",
    "                    spktrain=spktrain[unit_used, :], x_t=x_t, y_t=y_t, hd_t=hd_t, \n",
    "                    theta_t=theta_t, eeg_t=eeg_t, sample_bin=sample_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit 2D GP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heatmap_GP(rate_model, xlim, unit_used, xtheta_rate=None, eval_type='mean'):\n",
    "    # run_type: 0 for L_R, 1 for R_L\n",
    "    \n",
    "    def make_func(neuron):\n",
    "        def func_GP(pos):\n",
    "            prevshape = pos.shape[1:]\n",
    "            x = pos[0].flatten()\n",
    "            theta = pos[1].flatten()\n",
    "            covariates = [x, theta]\n",
    "            lower, mean, upper = rate_model.eval_rate(covariates, [neuron], 'posterior')\n",
    "            if eval_type == 'mean':\n",
    "                return mean[0].reshape(*prevshape)\n",
    "            elif eval_type == 'lower':\n",
    "                return lower[0].reshape(*prevshape)\n",
    "            elif eval_type == 'upper':\n",
    "                return upper[0].reshape(*prevshape)\n",
    "                \n",
    "        return func_GP\n",
    "    \n",
    "    nrows = rate_model.out_dims\n",
    "    ncols = 2 if xtheta_rate is not None else 1\n",
    "    widths = [0.5] * ncols\n",
    "    heights = [1] * nrows\n",
    "    \n",
    "    fig = plt.figure(figsize=(4*ncols, 3*nrows))\n",
    "\n",
    "    spec = fig.add_gridspec(ncols=ncols, nrows=nrows, width_ratios=widths,\n",
    "                             height_ratios=heights)#, left=0., right=0.2, bottom=0., top=1.)\n",
    "    \n",
    "    for neuron in range(rate_model.out_dims):\n",
    "        if xtheta_rate is not None:\n",
    "            grid_shape = [xlim, [0, 4*np.pi]]\n",
    "            \n",
    "            ax = fig.add_subplot(spec[neuron, 1])\n",
    "            field = np.tile(xtheta_rate[neuron].T, (2,1)) # 0->0, 1->2 as 1 is stationary/end runs\n",
    "            im = utils.plot.visualize_field((fig, ax), field, grid_shape, ticktitle='firing rate (Hz)', aspect='auto')\n",
    "\n",
    "            ylabel = r'$\\theta$'\n",
    "            xlabel = r'$x$ (mm)'\n",
    "            utils.plot.decorate_ax(ax, xlabel=xlabel, ylabel=ylabel, xlim=xlim, ylim=[0, 4*np.pi],\n",
    "                                  xticks=np.linspace(xlim[0], xlim[1], 3), yticks=[0, 2*np.pi, 4*np.pi],\n",
    "                                  spines=[False, False, False, False])\n",
    "            ax.set_yticklabels([r'$0$', r'$2\\pi$', r'$4\\pi$'])\n",
    "            ax.set_title(\"Histogram Neuron {}\".format(unit_used[neuron]))\n",
    "            vmax = np.max(field)\n",
    "            \n",
    "        \n",
    "        grid_shape = [xlim, [0, 4*np.pi]]\n",
    "        grid_size = [40, 30]\n",
    "\n",
    "        ax = fig.add_subplot(spec[neuron, 0])\n",
    "        _, field = utils.plot.compute_mesh(grid_size, grid_shape, make_func(neuron))\n",
    "        im = utils.plot.visualize_field((fig, ax), field, grid_shape, ticktitle='firing rate (Hz)', aspect='auto') #, vmax=vmax)\n",
    "\n",
    "        ylabel = r'$\\theta$'\n",
    "        xlabel = r'$x$ (mm)'\n",
    "        utils.plot.decorate_ax(ax, xlabel=xlabel, ylabel=ylabel, xlim=xlim, ylim=[0, 4*np.pi],\n",
    "                          xticks=np.linspace(xlim[0], xlim[1], 3), yticks=[0, 2*np.pi, 4*np.pi],\n",
    "                          spines=[False, False, False, False])\n",
    "        ax.set_yticklabels([r'$0$', r'$2\\pi$', r'$4\\pi$'])\n",
    "        ax.set_title(\"GP Neuron {}\".format(unit_used[neuron]))\n",
    "            \n",
    "    plt.tight_layout(w_pad=1)\n",
    "    plt.show()\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_GP_2D(x_ts, theta_ts, spikes, left_x, right_x, dt, batch_size, lr):\n",
    "    neurons, sim_samples = np.shape(spikes)\n",
    "    \n",
    "    ### GP KERNEL ###\n",
    "    l_spat = 1.*np.array([np.ones(neurons)])\n",
    "    l_ang = 0.1*np.array([np.ones(neurons)])\n",
    "    v = np.ones(neurons)\n",
    "    kernels_tuples = [('variance', v), ('Matern32', 'euclid', l_spat), ('Matern32', 'torus', l_ang)]\n",
    "    \n",
    "    ### INDUCING POINTS ###\n",
    "    in_dims = 2\n",
    "    num_induc = 20\n",
    "    inducing_points = np.array([np.linspace(left_x, right_x, num_induc), \n",
    "                                np.random.rand(num_induc)*2*np.pi]).T[None, :, :].repeat(neurons, axis=0)\n",
    "    # repeat to get separate inducing points per neuron\n",
    "\n",
    "    ### GP MODEL ###\n",
    "    rate_model = mdl.nonparametrics.Gaussian_process(\n",
    "        in_dims, # input dimensions\n",
    "        neurons, # number of neurons\n",
    "        kernels_tuples, # kernels\n",
    "        inducing_points=inducing_points, # initial inducing points\n",
    "        mean=0.*np.ones((neurons)), # GP mean separate per neuron\n",
    "        learn_mean=True, # learnable mean\n",
    "        inv_link='exp', # inverse link function\n",
    "        whiten=True, # use whitened representation\n",
    "        jitter=1e-4 # jitter (numerical stability) \n",
    "    )\n",
    "\n",
    "    ### LIKELIHOOD ###\n",
    "    likelihood = mdl.likelihoods.Poisson(dt, neurons, 'exp')\n",
    "    likelihood.set_Y(spikes, batch_size=batch_size, filter_len=1)\n",
    "\n",
    "    ### INPUTS ###\n",
    "    covariates = [x_ts.flatten(), utils.signal.WrapPi(theta_ts.flatten(), True)] # 0 to 2*pi\n",
    "    VI_tuples = [(None, None, None, 1), (None, None, None, 1)] # no latent variables\n",
    "    inputs = mdl.inference.input_group(in_dims, VI_tuples)\n",
    "    inputs.set_XZ(covariates, resamples, batch_size=batch_size, filter_len=1)\n",
    "   \n",
    "\n",
    "    ### VI FRAMEWORK ###\n",
    "    glm = mdl.inference.VI_optimized(inputs, rate_model, likelihood)\n",
    "    glm.to(dev) # move to GPU if available\n",
    "\n",
    "    # training parameters\n",
    "    sch = lambda o: optim.lr_scheduler.MultiplicativeLR(o, lambda e: 0.9)\n",
    "    opt_tuple = (optim.Adam, 100, sch)\n",
    "    opt_lr_dict = {'default': lr}#, 'kernel.lengthscale': 5*1e-3}\n",
    "    glm.set_optimizers(opt_tuple, opt_lr_dict)\n",
    "    \n",
    "    return rate_model, glm\n",
    "\n",
    "\n",
    "\n",
    "def fit_GP_2D(x_ts, theta_ts, spikes, left_x, right_x, dt, batch_size, lr=1e-2):\n",
    "    rate_model, glm = setup_GP_2D(x_ts, theta_ts, spikes, left_x, right_x, dt, batch_size, lr=lr)\n",
    "\n",
    "    annealing = lambda x: 1.0#min(1.0, 0.001*x)\n",
    "    losses = glm.fit(500, loss_margin=-1e0, margin_epochs=50, kl_anneal_func=annealing, \n",
    "                     cov_samples=1, ll_samples=10)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(losses)\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('NLL per time sample')\n",
    "    plt.show()\n",
    "    \n",
    "    return rate_model, glm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spktrain:  (96, 4877928)\n",
      "x_t:  (4877928,)\n",
      "theta_t:  (4877928,)\n"
     ]
    }
   ],
   "source": [
    "print(\"spktrain: \", np.shape(spktrain))\n",
    "print(\"x_t: \", np.shape(x_t))\n",
    "print(\"theta_t: \", np.shape(theta_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reserved:  0.96468992\n",
      "Allocated:  3.584e-05\n",
      "Free:  3.330313216\n"
     ]
    }
   ],
   "source": [
    "t = torch.cuda.get_device_properties(0).total_memory\n",
    "r = torch.cuda.memory_reserved(0) \n",
    "a = torch.cuda.memory_allocated(0)\n",
    "f = t-(r-a)  # free inside reserved\n",
    "# print(\"Total: \", t/1e9)\n",
    "print(\"Reserved: \", r/1e9)\n",
    "print(\"Allocated: \", a/1e9)\n",
    "print(\"Free: \", f/1e9)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
